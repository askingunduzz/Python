# -*- coding: utf-8 -*-
"""W7ALE3.ipynb adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OqMneqiyKH2uODfq8-7AJBc1JraHxwVD

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/y-akbal/Tedu_Python_for_DS/blob/main/ALE/W7ALE3.ipynb)
"""

import numpy as np
import pickle
import sklearn
import seaborn as sns
from sklearn.model_selection import train_test_split

### Below we split our dataset, and fix the random_state in order to
### make sure that splitting is done always the same manner
### Please beware that this task is a regression task NOT a classification task,
### Please go to sklearn's web site and see the arguments of this functions
from sklearn.datasets import load_diabetes
X,y = load_diabetes(return_X_y= True);
X_train, X_test, y_train, y_test  = train_test_split(X, y, random_state = 10);

"""# For properties of the dataset see https://rowannicholls.github.io/python/data/sklearn_datasets/diabetes.html

# Below you will implement your own KNNRegressor object. Feel free to use the ones that we implemented during the lecture, There is no so much difference, the regressor case is even simpler, as we do not need to do polling, rather we shall be taking avarage of the K-nearest neighbors.
"""

class KNNRegressor:
    def __init__(self, n_neighbors = 5):
        pass
    def fit(self, X_train, y_train):
        pass
    def __predictsingle__(self, x_sample)->float:
        pass
    def predict(self, X_test:np.ndarray)->np.ndarray:
        pass

"""## Let's give it a try!!! (The following cell will check if you implemeted needed functions correctly)"""

## Graded Cell A12, do not change this cell as this will cause compiler to crash
### This cell will check if you set up the right attributes, and right methods
knn = KNNRegressor() ## we create an instance of this class
assert callable(knn.fit), "Implement fit method first!!!"
assert callable(knn.predict), "Implement predict method!!!"
assert callable(knn.__predictsingle__), "Implement __predictsingle__ method!!!"
knn.fit(X_train, y_train)
assert len(vars(knn)) >= 3, "Do you think that your fit method is really functional??"
print("Chill the hell out broooo, seems that you are doing good!!!!")
L_ = True
del(knn) ## Garbage collection is your friend!!!!

"""## You shall now implement $R^2$ function. To do so, you will need review what $R^2$ score does.
## See here: https://en.wikipedia.org/wiki/Coefficient_of_determination
"""

def r2_score(y_true:np.ndarray, y_pred:np.ndarray) -> float:
    pass

### Graded Cell A34, do not change any line of the code below as this may cause compiler to crash,
###
np.random.seed(10)
y_true = np.random.randn(1000)
y_pred = np.random.randn(1000)
assert not np.isclose(r2_score(y_true, y_pred), -0.7612976464497292), "Please mind the order!!!! y_true should be in the denominator"
assert np.isclose(r2_score(y_true, y_pred), -0.981139), "Watcha your implementation"
print("Gott it buddy!!!, Guido van Rossum is proud of you!!!")
L = True
del(y_true) ### Collect your garbage!!!
del(y_pred) ### Collect your garbage!!!

"""## Let's now check if our KNNregressor object does really well.

"""

### Graded Cell 35, Do not change any part of this cell!!!!
import tqdm
from sklearn.neighbors import KNeighborsRegressor
check_list = []
for i in tqdm.trange(2, 200):
    knn_ = KNeighborsRegressor(n_neighbors = i)
    knn = KNNRegressor(n_neighbors= i)
    knn_.fit(X_train, y_train)
    knn.fit(X_train,y_train)
    y_pred = knn.predict(X_test)
    check_list.append(r2_score(y_test, y_pred) == knn_.score(X_test, y_test))
assert np.all(check_list), "Check your implementation comrade!!!"
print("\n So far everyhing is gooooooood!!!!")
L___ = True

"""## Let's do a final check to see if you are doin' good!!!"""

def final_check():
    if L+L_+L___ == 3:
        print("Yeap you got it!!!")
    else:
        print("Something went wrong :...(")
final_check()