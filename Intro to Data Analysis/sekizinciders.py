# -*- coding: utf-8 -*-
"""sekizinciDers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y5ZaXy9Q8n49maNySGusCXAc4YqHa9ie
"""

## implement your own reniagressor?
import numpy as np
import matplotlib.pyplot as plt
f = lambda x:x**3- x
## 3 yerine bi sayı yazdığında fraklı bi grafik çıkabilirdi dito hoca, 2 local minimums olurdu. böyle çok zor olurdu çözmek

plt.plot(f(np.arange(-1,1,0.1)))
##finding local maximum and minimum

f = lambda x:x[0]**2 + x[1]**2 - x[0] - x[1]   ## THİS FUNCTOİN HAS DOMAİN R^2
##you can find their local min max by using partial derivative too

##neural network, many machine learning problems

##minimize same function wrt same parameters!! :: called an optimization problem

##our task is to solve such optimization problams

grad_f = lambda x: np.array([2*x[0]-1, 2*x[1]-1]) ##what is gradient

x_init = np.random.randn(2)
##lr=2  çok büyük so lr=0.1filan kullanıoyourz
##hyperparameter
##step sizes. 0.1 0.1 sağa sola gidiyoruz

x_init

f(x_init)

for _ in range[10000]:  ##learning rate?  ## 10bin kere itere ediyorum
    x_init -= lr*grad_f(x_init)
    if i%100 == 0:
      print(f(x_init))

     ##gradyana bak ve gradyanın ters istikametine doğru git bla
## minimum value yı bulduk derivaive almadan
## gradient descending kullandık

##linnear reggession by menas of regressor class
def f(x):
  if x<=2:   ##çoktan seçmeli 10 soru
    return 1
  else:  ##fonksiyonun şu kısmını hangi şık olsun

    return f(x-1)+2*f(x-2)      ##buna güzel soru dedi
    ##diğer kısmı kod sorusu, temel şeyleri ölçücek

f(5)     ##1saat   inheritance! bu tarz şeyler dahil değil dedi
## numpy ama obilr
##ordaki fonksiyonları bilmeniz önemli dedi
##arrayi sort edelim

##bi parçası lms üzerinde  bi parçsı -- üzerinden olcak mail atıcak hoca
##birazcık daha kod yazmanızı isticem boşluk doldurmaca ama biraz zorlar beni aga
##hangi fonksioyn neyi yapıyo bil. sınav boyunca internete bakbirsin
#turnitin var ag

##lets implement our ownregresision oebjtec (multidimensional)
##multiple inheritance
##initialize fit predict

##by gradient distant filan bi şeyler dedi
##B0 B1 B2 *x1 *x2 ++

class LR:   ##bu fit predict filan bunarlı öğren mkk
  def __init__(self):
    self.loss=[]
    self.compiled=False

  def fit(self,X,y):
    self.shape=X.shape
    if not self.compiled:
      self.W=np.random.randn(shape[1]+1,1)   ##trueyse oluştur
      self.compiled=True  #bi kere girip bi daha girmciek  artık sadece aşağı iniyo artmıyor
            ##bunu oluştuyorumuş:np.hstack([np.ones((10,1)),X_features]) @B ##horizontal stack
    self.X=np.hstack([np.ones((shape[0],1),X)])
    for _ in range(n_iter):
      self.W -= lr*(self.X.T)@(self.X@ self.W - y)   ##loss fonksiyonunun gradyantı   ##bunu kimse yapmaz hocaaçıklamadı , otogradyant
##gradyantın tersiistikaminde küçk bi adım atıyo, update ediyo sürkeli aynısı
  def predict(self,X):
    return np.hstack([np.ones((shape[0],1),X)]) @ W
  def get_grad()
  def loss(self,X,y):
    y_pred= self.predict(X)
    return np.linealg.norm(y_pred-y)

X=np.random.randn(10,3)
y=np.random.randn(10,1)

lr=LR()
lr.fit(X,y,n_iter=10000,lr=0.0001)
#lr.fit(X_features,np.random.randn(10,1))

lr.predict(X,y)

lr.loss(X,y)

lr.W

X_features = np.random.randn(10,5)

X_features      ## 10 by 5

##B0 + B1*X1+B2*X2+B3*X3 ... +B5*X5
##We implement this using numpy
##matrix multiplication
##vektörizasyon

##10 samples 5 wjer
##we need 6 betas
B = np.random.randn(6,1)

B  ## 6 by 1

X_features @ B

np.hstack([np.ones((10,1)),X_features]) @B ##horizontal stack

##şimdi elimiz düzgün bi dataset oluşturucak bakıcaz 0a çekebiliyo muuyuz

X=np.random.randn(10,3)
W=np.random.randn(3,1)
b=np.random.randn()
y=X @ W +b

lr=LR()
lr.fit(X,y,n_iter=10000,lr=0.0001)
lr.__loss__(X,y)

lr.W  #way wtf

